{
  "best_global_step": 70929,
  "best_metric": 0.41345763206481934,
  "best_model_checkpoint": "./koelectra_hate_speech_lora/checkpoint-70929",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 70929,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.021147908471852134,
      "grad_norm": 1.143932819366455,
      "learning_rate": 1.9859295915633943e-05,
      "loss": 0.6597,
      "step": 500
    },
    {
      "epoch": 0.04229581694370427,
      "grad_norm": 1.7063313722610474,
      "learning_rate": 1.9718309859154933e-05,
      "loss": 0.5478,
      "step": 1000
    },
    {
      "epoch": 0.0634437254155564,
      "grad_norm": 1.3975820541381836,
      "learning_rate": 1.9577323802675915e-05,
      "loss": 0.5122,
      "step": 1500
    },
    {
      "epoch": 0.08459163388740853,
      "grad_norm": 3.4086711406707764,
      "learning_rate": 1.9436337746196904e-05,
      "loss": 0.5001,
      "step": 2000
    },
    {
      "epoch": 0.10573954235926067,
      "grad_norm": 3.09761381149292,
      "learning_rate": 1.929535168971789e-05,
      "loss": 0.4918,
      "step": 2500
    },
    {
      "epoch": 0.1268874508311128,
      "grad_norm": 1.5773547887802124,
      "learning_rate": 1.9154365633238875e-05,
      "loss": 0.4933,
      "step": 3000
    },
    {
      "epoch": 0.14803535930296494,
      "grad_norm": 2.403424024581909,
      "learning_rate": 1.901337957675986e-05,
      "loss": 0.4814,
      "step": 3500
    },
    {
      "epoch": 0.16918326777481707,
      "grad_norm": 2.9427683353424072,
      "learning_rate": 1.8872393520280847e-05,
      "loss": 0.4862,
      "step": 4000
    },
    {
      "epoch": 0.1903311762466692,
      "grad_norm": 7.182595252990723,
      "learning_rate": 1.8731407463801832e-05,
      "loss": 0.4763,
      "step": 4500
    },
    {
      "epoch": 0.21147908471852134,
      "grad_norm": 3.5194878578186035,
      "learning_rate": 1.8590421407322815e-05,
      "loss": 0.4865,
      "step": 5000
    },
    {
      "epoch": 0.23262699319037347,
      "grad_norm": 1.5854963064193726,
      "learning_rate": 1.8449435350843804e-05,
      "loss": 0.4693,
      "step": 5500
    },
    {
      "epoch": 0.2537749016622256,
      "grad_norm": 6.02665901184082,
      "learning_rate": 1.830844929436479e-05,
      "loss": 0.4687,
      "step": 6000
    },
    {
      "epoch": 0.27492281013407777,
      "grad_norm": 2.388073444366455,
      "learning_rate": 1.8167463237885775e-05,
      "loss": 0.4657,
      "step": 6500
    },
    {
      "epoch": 0.2960707186059299,
      "grad_norm": 4.400915622711182,
      "learning_rate": 1.802647718140676e-05,
      "loss": 0.4729,
      "step": 7000
    },
    {
      "epoch": 0.31721862707778203,
      "grad_norm": 1.6340464353561401,
      "learning_rate": 1.7885491124927746e-05,
      "loss": 0.4617,
      "step": 7500
    },
    {
      "epoch": 0.33836653554963414,
      "grad_norm": 3.3207879066467285,
      "learning_rate": 1.7744505068448732e-05,
      "loss": 0.4676,
      "step": 8000
    },
    {
      "epoch": 0.3595144440214863,
      "grad_norm": 1.9794672727584839,
      "learning_rate": 1.7603519011969718e-05,
      "loss": 0.4542,
      "step": 8500
    },
    {
      "epoch": 0.3806623524933384,
      "grad_norm": 1.7508924007415771,
      "learning_rate": 1.7462532955490703e-05,
      "loss": 0.4522,
      "step": 9000
    },
    {
      "epoch": 0.40181026096519057,
      "grad_norm": 5.006061553955078,
      "learning_rate": 1.732154689901169e-05,
      "loss": 0.4593,
      "step": 9500
    },
    {
      "epoch": 0.4229581694370427,
      "grad_norm": 6.121144771575928,
      "learning_rate": 1.7180560842532675e-05,
      "loss": 0.4548,
      "step": 10000
    },
    {
      "epoch": 0.44410607790889484,
      "grad_norm": 2.553002119064331,
      "learning_rate": 1.703957478605366e-05,
      "loss": 0.4422,
      "step": 10500
    },
    {
      "epoch": 0.46525398638074694,
      "grad_norm": 3.4548957347869873,
      "learning_rate": 1.6898588729574646e-05,
      "loss": 0.4528,
      "step": 11000
    },
    {
      "epoch": 0.4864018948525991,
      "grad_norm": 3.024893045425415,
      "learning_rate": 1.6757602673095632e-05,
      "loss": 0.4461,
      "step": 11500
    },
    {
      "epoch": 0.5075498033244512,
      "grad_norm": 2.3346760272979736,
      "learning_rate": 1.6616616616616618e-05,
      "loss": 0.4657,
      "step": 12000
    },
    {
      "epoch": 0.5286977117963033,
      "grad_norm": 2.6546714305877686,
      "learning_rate": 1.6475630560137603e-05,
      "loss": 0.4508,
      "step": 12500
    },
    {
      "epoch": 0.5498456202681555,
      "grad_norm": 2.565140962600708,
      "learning_rate": 1.633464450365859e-05,
      "loss": 0.4513,
      "step": 13000
    },
    {
      "epoch": 0.5709935287400076,
      "grad_norm": 3.4334664344787598,
      "learning_rate": 1.6193658447179575e-05,
      "loss": 0.4401,
      "step": 13500
    },
    {
      "epoch": 0.5921414372118597,
      "grad_norm": 4.946577548980713,
      "learning_rate": 1.605267239070056e-05,
      "loss": 0.44,
      "step": 14000
    },
    {
      "epoch": 0.6132893456837119,
      "grad_norm": 1.7721806764602661,
      "learning_rate": 1.591168633422155e-05,
      "loss": 0.4418,
      "step": 14500
    },
    {
      "epoch": 0.6344372541555641,
      "grad_norm": 2.4196999073028564,
      "learning_rate": 1.577070027774253e-05,
      "loss": 0.4526,
      "step": 15000
    },
    {
      "epoch": 0.6555851626274162,
      "grad_norm": 3.050539016723633,
      "learning_rate": 1.5629714221263517e-05,
      "loss": 0.445,
      "step": 15500
    },
    {
      "epoch": 0.6767330710992683,
      "grad_norm": 2.2407002449035645,
      "learning_rate": 1.5488728164784503e-05,
      "loss": 0.4491,
      "step": 16000
    },
    {
      "epoch": 0.6978809795711204,
      "grad_norm": 4.886918544769287,
      "learning_rate": 1.534774210830549e-05,
      "loss": 0.4599,
      "step": 16500
    },
    {
      "epoch": 0.7190288880429726,
      "grad_norm": 3.7885563373565674,
      "learning_rate": 1.5206756051826474e-05,
      "loss": 0.4404,
      "step": 17000
    },
    {
      "epoch": 0.7401767965148247,
      "grad_norm": 3.597593307495117,
      "learning_rate": 1.5065769995347462e-05,
      "loss": 0.4471,
      "step": 17500
    },
    {
      "epoch": 0.7613247049866768,
      "grad_norm": 5.635378837585449,
      "learning_rate": 1.4924783938868447e-05,
      "loss": 0.451,
      "step": 18000
    },
    {
      "epoch": 0.7824726134585289,
      "grad_norm": 2.09614896774292,
      "learning_rate": 1.4783797882389433e-05,
      "loss": 0.4421,
      "step": 18500
    },
    {
      "epoch": 0.8036205219303811,
      "grad_norm": 1.9858288764953613,
      "learning_rate": 1.4642811825910419e-05,
      "loss": 0.4568,
      "step": 19000
    },
    {
      "epoch": 0.8247684304022332,
      "grad_norm": 3.486679792404175,
      "learning_rate": 1.4501825769431403e-05,
      "loss": 0.4486,
      "step": 19500
    },
    {
      "epoch": 0.8459163388740853,
      "grad_norm": 2.8959381580352783,
      "learning_rate": 1.436083971295239e-05,
      "loss": 0.4516,
      "step": 20000
    },
    {
      "epoch": 0.8670642473459375,
      "grad_norm": 4.286369800567627,
      "learning_rate": 1.4219853656473378e-05,
      "loss": 0.4336,
      "step": 20500
    },
    {
      "epoch": 0.8882121558177897,
      "grad_norm": 2.7780303955078125,
      "learning_rate": 1.4078867599994362e-05,
      "loss": 0.4426,
      "step": 21000
    },
    {
      "epoch": 0.9093600642896418,
      "grad_norm": 1.9510722160339355,
      "learning_rate": 1.3937881543515349e-05,
      "loss": 0.4482,
      "step": 21500
    },
    {
      "epoch": 0.9305079727614939,
      "grad_norm": 5.089450359344482,
      "learning_rate": 1.3796895487036333e-05,
      "loss": 0.4455,
      "step": 22000
    },
    {
      "epoch": 0.951655881233346,
      "grad_norm": 3.3516769409179688,
      "learning_rate": 1.3655909430557319e-05,
      "loss": 0.4306,
      "step": 22500
    },
    {
      "epoch": 0.9728037897051982,
      "grad_norm": 5.436390399932861,
      "learning_rate": 1.3514923374078304e-05,
      "loss": 0.4371,
      "step": 23000
    },
    {
      "epoch": 0.9939516981770503,
      "grad_norm": 3.571840763092041,
      "learning_rate": 1.337393731759929e-05,
      "loss": 0.4392,
      "step": 23500
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.8176784576163161,
      "eval_loss": 0.4282248616218567,
      "eval_runtime": 127.0488,
      "eval_samples_per_second": 372.18,
      "eval_steps_per_second": 23.267,
      "step": 23643
    },
    {
      "epoch": 1.0150996066489024,
      "grad_norm": 1.8417049646377563,
      "learning_rate": 1.3232951261120277e-05,
      "loss": 0.4355,
      "step": 24000
    },
    {
      "epoch": 1.0362475151207546,
      "grad_norm": 1.8410229682922363,
      "learning_rate": 1.3091965204641261e-05,
      "loss": 0.4383,
      "step": 24500
    },
    {
      "epoch": 1.0573954235926066,
      "grad_norm": 2.55532169342041,
      "learning_rate": 1.2950979148162249e-05,
      "loss": 0.4297,
      "step": 25000
    },
    {
      "epoch": 1.0785433320644588,
      "grad_norm": 3.280797243118286,
      "learning_rate": 1.2809993091683233e-05,
      "loss": 0.4383,
      "step": 25500
    },
    {
      "epoch": 1.0996912405363108,
      "grad_norm": 1.1168664693832397,
      "learning_rate": 1.266900703520422e-05,
      "loss": 0.4321,
      "step": 26000
    },
    {
      "epoch": 1.120839149008163,
      "grad_norm": 2.9360032081604004,
      "learning_rate": 1.2528020978725206e-05,
      "loss": 0.4479,
      "step": 26500
    },
    {
      "epoch": 1.1419870574800153,
      "grad_norm": 1.2145196199417114,
      "learning_rate": 1.238703492224619e-05,
      "loss": 0.4366,
      "step": 27000
    },
    {
      "epoch": 1.1631349659518673,
      "grad_norm": 3.318023443222046,
      "learning_rate": 1.2246048865767177e-05,
      "loss": 0.4314,
      "step": 27500
    },
    {
      "epoch": 1.1842828744237195,
      "grad_norm": 3.893270969390869,
      "learning_rate": 1.2105062809288161e-05,
      "loss": 0.427,
      "step": 28000
    },
    {
      "epoch": 1.2054307828955717,
      "grad_norm": 4.853907585144043,
      "learning_rate": 1.1964076752809148e-05,
      "loss": 0.4365,
      "step": 28500
    },
    {
      "epoch": 1.2265786913674237,
      "grad_norm": 8.816139221191406,
      "learning_rate": 1.1823090696330132e-05,
      "loss": 0.4248,
      "step": 29000
    },
    {
      "epoch": 1.247726599839276,
      "grad_norm": 3.774414539337158,
      "learning_rate": 1.168210463985112e-05,
      "loss": 0.4297,
      "step": 29500
    },
    {
      "epoch": 1.2688745083111281,
      "grad_norm": 2.367023468017578,
      "learning_rate": 1.1541118583372105e-05,
      "loss": 0.4289,
      "step": 30000
    },
    {
      "epoch": 1.2900224167829801,
      "grad_norm": 3.091655969619751,
      "learning_rate": 1.1400132526893091e-05,
      "loss": 0.4276,
      "step": 30500
    },
    {
      "epoch": 1.3111703252548323,
      "grad_norm": 3.6124284267425537,
      "learning_rate": 1.1259146470414077e-05,
      "loss": 0.428,
      "step": 31000
    },
    {
      "epoch": 1.3323182337266846,
      "grad_norm": 3.155005693435669,
      "learning_rate": 1.1118160413935063e-05,
      "loss": 0.4456,
      "step": 31500
    },
    {
      "epoch": 1.3534661421985366,
      "grad_norm": 2.377854347229004,
      "learning_rate": 1.0977174357456048e-05,
      "loss": 0.4387,
      "step": 32000
    },
    {
      "epoch": 1.3746140506703888,
      "grad_norm": 3.1064321994781494,
      "learning_rate": 1.0836188300977036e-05,
      "loss": 0.4244,
      "step": 32500
    },
    {
      "epoch": 1.3957619591422408,
      "grad_norm": 2.801030158996582,
      "learning_rate": 1.069520224449802e-05,
      "loss": 0.4376,
      "step": 33000
    },
    {
      "epoch": 1.416909867614093,
      "grad_norm": 2.817599296569824,
      "learning_rate": 1.0554216188019007e-05,
      "loss": 0.426,
      "step": 33500
    },
    {
      "epoch": 1.438057776085945,
      "grad_norm": 3.1364099979400635,
      "learning_rate": 1.0413230131539991e-05,
      "loss": 0.4305,
      "step": 34000
    },
    {
      "epoch": 1.4592056845577972,
      "grad_norm": 3.0952930450439453,
      "learning_rate": 1.0272244075060978e-05,
      "loss": 0.4241,
      "step": 34500
    },
    {
      "epoch": 1.4803535930296494,
      "grad_norm": 3.3036043643951416,
      "learning_rate": 1.0131258018581962e-05,
      "loss": 0.4443,
      "step": 35000
    },
    {
      "epoch": 1.5015015015015014,
      "grad_norm": 5.927052974700928,
      "learning_rate": 9.990271962102948e-06,
      "loss": 0.4268,
      "step": 35500
    },
    {
      "epoch": 1.5226494099733536,
      "grad_norm": 2.4193713665008545,
      "learning_rate": 9.849285905623934e-06,
      "loss": 0.4264,
      "step": 36000
    },
    {
      "epoch": 1.5437973184452058,
      "grad_norm": 3.6692991256713867,
      "learning_rate": 9.70829984914492e-06,
      "loss": 0.4262,
      "step": 36500
    },
    {
      "epoch": 1.5649452269170578,
      "grad_norm": 4.1695556640625,
      "learning_rate": 9.567313792665907e-06,
      "loss": 0.4262,
      "step": 37000
    },
    {
      "epoch": 1.58609313538891,
      "grad_norm": 4.721271514892578,
      "learning_rate": 9.426327736186892e-06,
      "loss": 0.427,
      "step": 37500
    },
    {
      "epoch": 1.6072410438607623,
      "grad_norm": 5.748585224151611,
      "learning_rate": 9.285341679707878e-06,
      "loss": 0.4237,
      "step": 38000
    },
    {
      "epoch": 1.6283889523326143,
      "grad_norm": 3.3262226581573486,
      "learning_rate": 9.144355623228864e-06,
      "loss": 0.4178,
      "step": 38500
    },
    {
      "epoch": 1.6495368608044665,
      "grad_norm": 3.021646499633789,
      "learning_rate": 9.00336956674985e-06,
      "loss": 0.425,
      "step": 39000
    },
    {
      "epoch": 1.6706847692763187,
      "grad_norm": 2.8362009525299072,
      "learning_rate": 8.862383510270835e-06,
      "loss": 0.4226,
      "step": 39500
    },
    {
      "epoch": 1.6918326777481707,
      "grad_norm": 4.430587291717529,
      "learning_rate": 8.72139745379182e-06,
      "loss": 0.4368,
      "step": 40000
    },
    {
      "epoch": 1.7129805862200227,
      "grad_norm": 5.807872295379639,
      "learning_rate": 8.580411397312807e-06,
      "loss": 0.4168,
      "step": 40500
    },
    {
      "epoch": 1.7341284946918751,
      "grad_norm": 6.695135116577148,
      "learning_rate": 8.439425340833792e-06,
      "loss": 0.4261,
      "step": 41000
    },
    {
      "epoch": 1.7552764031637271,
      "grad_norm": 4.665721416473389,
      "learning_rate": 8.298439284354778e-06,
      "loss": 0.426,
      "step": 41500
    },
    {
      "epoch": 1.7764243116355791,
      "grad_norm": 2.689242362976074,
      "learning_rate": 8.157453227875764e-06,
      "loss": 0.4255,
      "step": 42000
    },
    {
      "epoch": 1.7975722201074313,
      "grad_norm": 2.661057472229004,
      "learning_rate": 8.01646717139675e-06,
      "loss": 0.4264,
      "step": 42500
    },
    {
      "epoch": 1.8187201285792836,
      "grad_norm": 1.9244905710220337,
      "learning_rate": 7.875481114917735e-06,
      "loss": 0.4248,
      "step": 43000
    },
    {
      "epoch": 1.8398680370511356,
      "grad_norm": 1.9605824947357178,
      "learning_rate": 7.73449505843872e-06,
      "loss": 0.4313,
      "step": 43500
    },
    {
      "epoch": 1.8610159455229878,
      "grad_norm": 3.144864082336426,
      "learning_rate": 7.593509001959707e-06,
      "loss": 0.4278,
      "step": 44000
    },
    {
      "epoch": 1.88216385399484,
      "grad_norm": 2.347815990447998,
      "learning_rate": 7.452522945480693e-06,
      "loss": 0.4272,
      "step": 44500
    },
    {
      "epoch": 1.903311762466692,
      "grad_norm": 4.2547831535339355,
      "learning_rate": 7.311536889001678e-06,
      "loss": 0.4113,
      "step": 45000
    },
    {
      "epoch": 1.9244596709385442,
      "grad_norm": 3.2410504817962646,
      "learning_rate": 7.170550832522663e-06,
      "loss": 0.4318,
      "step": 45500
    },
    {
      "epoch": 1.9456075794103964,
      "grad_norm": 2.6696250438690186,
      "learning_rate": 7.029564776043651e-06,
      "loss": 0.4233,
      "step": 46000
    },
    {
      "epoch": 1.9667554878822484,
      "grad_norm": 5.767545700073242,
      "learning_rate": 6.8885787195646355e-06,
      "loss": 0.4319,
      "step": 46500
    },
    {
      "epoch": 1.9879033963541006,
      "grad_norm": 2.764876127243042,
      "learning_rate": 6.747592663085621e-06,
      "loss": 0.4301,
      "step": 47000
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.824327263380588,
      "eval_loss": 0.41405877470970154,
      "eval_runtime": 125.3449,
      "eval_samples_per_second": 377.239,
      "eval_steps_per_second": 23.583,
      "step": 47286
    },
    {
      "epoch": 2.009051304825953,
      "grad_norm": 2.6661014556884766,
      "learning_rate": 6.606606606606607e-06,
      "loss": 0.421,
      "step": 47500
    },
    {
      "epoch": 2.030199213297805,
      "grad_norm": 2.086371898651123,
      "learning_rate": 6.465620550127593e-06,
      "loss": 0.4234,
      "step": 48000
    },
    {
      "epoch": 2.051347121769657,
      "grad_norm": 5.232722759246826,
      "learning_rate": 6.324634493648578e-06,
      "loss": 0.4329,
      "step": 48500
    },
    {
      "epoch": 2.0724950302415093,
      "grad_norm": 2.56068754196167,
      "learning_rate": 6.183648437169565e-06,
      "loss": 0.4244,
      "step": 49000
    },
    {
      "epoch": 2.0936429387133613,
      "grad_norm": 2.344102382659912,
      "learning_rate": 6.0426623806905505e-06,
      "loss": 0.4266,
      "step": 49500
    },
    {
      "epoch": 2.1147908471852133,
      "grad_norm": 4.534311771392822,
      "learning_rate": 5.901676324211536e-06,
      "loss": 0.4183,
      "step": 50000
    },
    {
      "epoch": 2.1359387556570657,
      "grad_norm": 4.545265197753906,
      "learning_rate": 5.760690267732522e-06,
      "loss": 0.4083,
      "step": 50500
    },
    {
      "epoch": 2.1570866641289177,
      "grad_norm": 2.2210302352905273,
      "learning_rate": 5.6197042112535075e-06,
      "loss": 0.4214,
      "step": 51000
    },
    {
      "epoch": 2.1782345726007697,
      "grad_norm": 4.855678081512451,
      "learning_rate": 5.478718154774492e-06,
      "loss": 0.4281,
      "step": 51500
    },
    {
      "epoch": 2.1993824810726217,
      "grad_norm": 4.056950569152832,
      "learning_rate": 5.33773209829548e-06,
      "loss": 0.4182,
      "step": 52000
    },
    {
      "epoch": 2.220530389544474,
      "grad_norm": 7.154744625091553,
      "learning_rate": 5.196746041816465e-06,
      "loss": 0.4206,
      "step": 52500
    },
    {
      "epoch": 2.241678298016326,
      "grad_norm": 3.3380801677703857,
      "learning_rate": 5.05575998533745e-06,
      "loss": 0.426,
      "step": 53000
    },
    {
      "epoch": 2.262826206488178,
      "grad_norm": 5.302800178527832,
      "learning_rate": 4.914773928858436e-06,
      "loss": 0.4202,
      "step": 53500
    },
    {
      "epoch": 2.2839741149600306,
      "grad_norm": 5.301421165466309,
      "learning_rate": 4.7737878723794225e-06,
      "loss": 0.4314,
      "step": 54000
    },
    {
      "epoch": 2.3051220234318825,
      "grad_norm": 3.1057980060577393,
      "learning_rate": 4.632801815900408e-06,
      "loss": 0.4284,
      "step": 54500
    },
    {
      "epoch": 2.3262699319037345,
      "grad_norm": 7.937801361083984,
      "learning_rate": 4.491815759421394e-06,
      "loss": 0.425,
      "step": 55000
    },
    {
      "epoch": 2.347417840375587,
      "grad_norm": 3.481142997741699,
      "learning_rate": 4.3508297029423795e-06,
      "loss": 0.4372,
      "step": 55500
    },
    {
      "epoch": 2.368565748847439,
      "grad_norm": 4.088016986846924,
      "learning_rate": 4.209843646463365e-06,
      "loss": 0.4133,
      "step": 56000
    },
    {
      "epoch": 2.389713657319291,
      "grad_norm": 2.4426379203796387,
      "learning_rate": 4.068857589984351e-06,
      "loss": 0.4144,
      "step": 56500
    },
    {
      "epoch": 2.4108615657911434,
      "grad_norm": 2.45719838142395,
      "learning_rate": 3.9278715335053366e-06,
      "loss": 0.4193,
      "step": 57000
    },
    {
      "epoch": 2.4320094742629954,
      "grad_norm": 4.296188831329346,
      "learning_rate": 3.7868854770263227e-06,
      "loss": 0.4114,
      "step": 57500
    },
    {
      "epoch": 2.4531573827348474,
      "grad_norm": 3.3673226833343506,
      "learning_rate": 3.645899420547308e-06,
      "loss": 0.4086,
      "step": 58000
    },
    {
      "epoch": 2.4743052912067,
      "grad_norm": 3.4883360862731934,
      "learning_rate": 3.504913364068294e-06,
      "loss": 0.4306,
      "step": 58500
    },
    {
      "epoch": 2.495453199678552,
      "grad_norm": 4.057122707366943,
      "learning_rate": 3.3639273075892797e-06,
      "loss": 0.419,
      "step": 59000
    },
    {
      "epoch": 2.516601108150404,
      "grad_norm": 5.400395393371582,
      "learning_rate": 3.2229412511102654e-06,
      "loss": 0.4187,
      "step": 59500
    },
    {
      "epoch": 2.5377490166222563,
      "grad_norm": 2.2109923362731934,
      "learning_rate": 3.0819551946312515e-06,
      "loss": 0.4278,
      "step": 60000
    },
    {
      "epoch": 2.5588969250941083,
      "grad_norm": 4.591512680053711,
      "learning_rate": 2.940969138152237e-06,
      "loss": 0.4173,
      "step": 60500
    },
    {
      "epoch": 2.5800448335659603,
      "grad_norm": 8.760270118713379,
      "learning_rate": 2.7999830816732224e-06,
      "loss": 0.4201,
      "step": 61000
    },
    {
      "epoch": 2.6011927420378127,
      "grad_norm": 4.011856555938721,
      "learning_rate": 2.658997025194209e-06,
      "loss": 0.4276,
      "step": 61500
    },
    {
      "epoch": 2.6223406505096647,
      "grad_norm": 2.618464469909668,
      "learning_rate": 2.5180109687151942e-06,
      "loss": 0.4246,
      "step": 62000
    },
    {
      "epoch": 2.6434885589815167,
      "grad_norm": 2.078230142593384,
      "learning_rate": 2.37702491223618e-06,
      "loss": 0.4201,
      "step": 62500
    },
    {
      "epoch": 2.664636467453369,
      "grad_norm": 5.127122402191162,
      "learning_rate": 2.2360388557571656e-06,
      "loss": 0.4134,
      "step": 63000
    },
    {
      "epoch": 2.685784375925221,
      "grad_norm": 6.701295852661133,
      "learning_rate": 2.0950527992781517e-06,
      "loss": 0.4201,
      "step": 63500
    },
    {
      "epoch": 2.706932284397073,
      "grad_norm": 4.350358486175537,
      "learning_rate": 1.9540667427991374e-06,
      "loss": 0.4201,
      "step": 64000
    },
    {
      "epoch": 2.728080192868925,
      "grad_norm": 2.7346863746643066,
      "learning_rate": 1.813080686320123e-06,
      "loss": 0.4172,
      "step": 64500
    },
    {
      "epoch": 2.7492281013407776,
      "grad_norm": 2.9448914527893066,
      "learning_rate": 1.672094629841109e-06,
      "loss": 0.4184,
      "step": 65000
    },
    {
      "epoch": 2.7703760098126295,
      "grad_norm": 4.0579423904418945,
      "learning_rate": 1.5311085733620944e-06,
      "loss": 0.4228,
      "step": 65500
    },
    {
      "epoch": 2.7915239182844815,
      "grad_norm": 3.6017210483551025,
      "learning_rate": 1.3901225168830803e-06,
      "loss": 0.4078,
      "step": 66000
    },
    {
      "epoch": 2.8126718267563335,
      "grad_norm": 3.2523093223571777,
      "learning_rate": 1.249136460404066e-06,
      "loss": 0.4157,
      "step": 66500
    },
    {
      "epoch": 2.833819735228186,
      "grad_norm": 3.5189430713653564,
      "learning_rate": 1.108150403925052e-06,
      "loss": 0.4238,
      "step": 67000
    },
    {
      "epoch": 2.854967643700038,
      "grad_norm": 6.34766149520874,
      "learning_rate": 9.671643474460376e-07,
      "loss": 0.4156,
      "step": 67500
    },
    {
      "epoch": 2.87611555217189,
      "grad_norm": 4.047536373138428,
      "learning_rate": 8.261782909670234e-07,
      "loss": 0.4189,
      "step": 68000
    },
    {
      "epoch": 2.8972634606437424,
      "grad_norm": 3.242476224899292,
      "learning_rate": 6.851922344880093e-07,
      "loss": 0.4231,
      "step": 68500
    },
    {
      "epoch": 2.9184113691155944,
      "grad_norm": 4.3702287673950195,
      "learning_rate": 5.442061780089949e-07,
      "loss": 0.4124,
      "step": 69000
    },
    {
      "epoch": 2.9395592775874464,
      "grad_norm": 4.1638503074646,
      "learning_rate": 4.0322012152998073e-07,
      "loss": 0.4159,
      "step": 69500
    },
    {
      "epoch": 2.960707186059299,
      "grad_norm": 3.6704044342041016,
      "learning_rate": 2.6223406505096646e-07,
      "loss": 0.4227,
      "step": 70000
    },
    {
      "epoch": 2.981855094531151,
      "grad_norm": 2.0131800174713135,
      "learning_rate": 1.2124800857195223e-07,
      "loss": 0.4229,
      "step": 70500
    },
    {
      "epoch": 3.0,
      "eval_f1": 0.8212323351108047,
      "eval_loss": 0.41345763206481934,
      "eval_runtime": 114.9627,
      "eval_samples_per_second": 411.307,
      "eval_steps_per_second": 25.713,
      "step": 70929
    }
  ],
  "logging_steps": 500,
  "max_steps": 70929,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0547358615414115e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
