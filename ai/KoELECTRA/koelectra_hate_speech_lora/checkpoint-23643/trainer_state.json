{
  "best_global_step": 23643,
  "best_metric": 0.4282248616218567,
  "best_model_checkpoint": "./koelectra_hate_speech_lora/checkpoint-23643",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 23643,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.021147908471852134,
      "grad_norm": 1.143932819366455,
      "learning_rate": 1.9859295915633943e-05,
      "loss": 0.6597,
      "step": 500
    },
    {
      "epoch": 0.04229581694370427,
      "grad_norm": 1.7063313722610474,
      "learning_rate": 1.9718309859154933e-05,
      "loss": 0.5478,
      "step": 1000
    },
    {
      "epoch": 0.0634437254155564,
      "grad_norm": 1.3975820541381836,
      "learning_rate": 1.9577323802675915e-05,
      "loss": 0.5122,
      "step": 1500
    },
    {
      "epoch": 0.08459163388740853,
      "grad_norm": 3.4086711406707764,
      "learning_rate": 1.9436337746196904e-05,
      "loss": 0.5001,
      "step": 2000
    },
    {
      "epoch": 0.10573954235926067,
      "grad_norm": 3.09761381149292,
      "learning_rate": 1.929535168971789e-05,
      "loss": 0.4918,
      "step": 2500
    },
    {
      "epoch": 0.1268874508311128,
      "grad_norm": 1.5773547887802124,
      "learning_rate": 1.9154365633238875e-05,
      "loss": 0.4933,
      "step": 3000
    },
    {
      "epoch": 0.14803535930296494,
      "grad_norm": 2.403424024581909,
      "learning_rate": 1.901337957675986e-05,
      "loss": 0.4814,
      "step": 3500
    },
    {
      "epoch": 0.16918326777481707,
      "grad_norm": 2.9427683353424072,
      "learning_rate": 1.8872393520280847e-05,
      "loss": 0.4862,
      "step": 4000
    },
    {
      "epoch": 0.1903311762466692,
      "grad_norm": 7.182595252990723,
      "learning_rate": 1.8731407463801832e-05,
      "loss": 0.4763,
      "step": 4500
    },
    {
      "epoch": 0.21147908471852134,
      "grad_norm": 3.5194878578186035,
      "learning_rate": 1.8590421407322815e-05,
      "loss": 0.4865,
      "step": 5000
    },
    {
      "epoch": 0.23262699319037347,
      "grad_norm": 1.5854963064193726,
      "learning_rate": 1.8449435350843804e-05,
      "loss": 0.4693,
      "step": 5500
    },
    {
      "epoch": 0.2537749016622256,
      "grad_norm": 6.02665901184082,
      "learning_rate": 1.830844929436479e-05,
      "loss": 0.4687,
      "step": 6000
    },
    {
      "epoch": 0.27492281013407777,
      "grad_norm": 2.388073444366455,
      "learning_rate": 1.8167463237885775e-05,
      "loss": 0.4657,
      "step": 6500
    },
    {
      "epoch": 0.2960707186059299,
      "grad_norm": 4.400915622711182,
      "learning_rate": 1.802647718140676e-05,
      "loss": 0.4729,
      "step": 7000
    },
    {
      "epoch": 0.31721862707778203,
      "grad_norm": 1.6340464353561401,
      "learning_rate": 1.7885491124927746e-05,
      "loss": 0.4617,
      "step": 7500
    },
    {
      "epoch": 0.33836653554963414,
      "grad_norm": 3.3207879066467285,
      "learning_rate": 1.7744505068448732e-05,
      "loss": 0.4676,
      "step": 8000
    },
    {
      "epoch": 0.3595144440214863,
      "grad_norm": 1.9794672727584839,
      "learning_rate": 1.7603519011969718e-05,
      "loss": 0.4542,
      "step": 8500
    },
    {
      "epoch": 0.3806623524933384,
      "grad_norm": 1.7508924007415771,
      "learning_rate": 1.7462532955490703e-05,
      "loss": 0.4522,
      "step": 9000
    },
    {
      "epoch": 0.40181026096519057,
      "grad_norm": 5.006061553955078,
      "learning_rate": 1.732154689901169e-05,
      "loss": 0.4593,
      "step": 9500
    },
    {
      "epoch": 0.4229581694370427,
      "grad_norm": 6.121144771575928,
      "learning_rate": 1.7180560842532675e-05,
      "loss": 0.4548,
      "step": 10000
    },
    {
      "epoch": 0.44410607790889484,
      "grad_norm": 2.553002119064331,
      "learning_rate": 1.703957478605366e-05,
      "loss": 0.4422,
      "step": 10500
    },
    {
      "epoch": 0.46525398638074694,
      "grad_norm": 3.4548957347869873,
      "learning_rate": 1.6898588729574646e-05,
      "loss": 0.4528,
      "step": 11000
    },
    {
      "epoch": 0.4864018948525991,
      "grad_norm": 3.024893045425415,
      "learning_rate": 1.6757602673095632e-05,
      "loss": 0.4461,
      "step": 11500
    },
    {
      "epoch": 0.5075498033244512,
      "grad_norm": 2.3346760272979736,
      "learning_rate": 1.6616616616616618e-05,
      "loss": 0.4657,
      "step": 12000
    },
    {
      "epoch": 0.5286977117963033,
      "grad_norm": 2.6546714305877686,
      "learning_rate": 1.6475630560137603e-05,
      "loss": 0.4508,
      "step": 12500
    },
    {
      "epoch": 0.5498456202681555,
      "grad_norm": 2.565140962600708,
      "learning_rate": 1.633464450365859e-05,
      "loss": 0.4513,
      "step": 13000
    },
    {
      "epoch": 0.5709935287400076,
      "grad_norm": 3.4334664344787598,
      "learning_rate": 1.6193658447179575e-05,
      "loss": 0.4401,
      "step": 13500
    },
    {
      "epoch": 0.5921414372118597,
      "grad_norm": 4.946577548980713,
      "learning_rate": 1.605267239070056e-05,
      "loss": 0.44,
      "step": 14000
    },
    {
      "epoch": 0.6132893456837119,
      "grad_norm": 1.7721806764602661,
      "learning_rate": 1.591168633422155e-05,
      "loss": 0.4418,
      "step": 14500
    },
    {
      "epoch": 0.6344372541555641,
      "grad_norm": 2.4196999073028564,
      "learning_rate": 1.577070027774253e-05,
      "loss": 0.4526,
      "step": 15000
    },
    {
      "epoch": 0.6555851626274162,
      "grad_norm": 3.050539016723633,
      "learning_rate": 1.5629714221263517e-05,
      "loss": 0.445,
      "step": 15500
    },
    {
      "epoch": 0.6767330710992683,
      "grad_norm": 2.2407002449035645,
      "learning_rate": 1.5488728164784503e-05,
      "loss": 0.4491,
      "step": 16000
    },
    {
      "epoch": 0.6978809795711204,
      "grad_norm": 4.886918544769287,
      "learning_rate": 1.534774210830549e-05,
      "loss": 0.4599,
      "step": 16500
    },
    {
      "epoch": 0.7190288880429726,
      "grad_norm": 3.7885563373565674,
      "learning_rate": 1.5206756051826474e-05,
      "loss": 0.4404,
      "step": 17000
    },
    {
      "epoch": 0.7401767965148247,
      "grad_norm": 3.597593307495117,
      "learning_rate": 1.5065769995347462e-05,
      "loss": 0.4471,
      "step": 17500
    },
    {
      "epoch": 0.7613247049866768,
      "grad_norm": 5.635378837585449,
      "learning_rate": 1.4924783938868447e-05,
      "loss": 0.451,
      "step": 18000
    },
    {
      "epoch": 0.7824726134585289,
      "grad_norm": 2.09614896774292,
      "learning_rate": 1.4783797882389433e-05,
      "loss": 0.4421,
      "step": 18500
    },
    {
      "epoch": 0.8036205219303811,
      "grad_norm": 1.9858288764953613,
      "learning_rate": 1.4642811825910419e-05,
      "loss": 0.4568,
      "step": 19000
    },
    {
      "epoch": 0.8247684304022332,
      "grad_norm": 3.486679792404175,
      "learning_rate": 1.4501825769431403e-05,
      "loss": 0.4486,
      "step": 19500
    },
    {
      "epoch": 0.8459163388740853,
      "grad_norm": 2.8959381580352783,
      "learning_rate": 1.436083971295239e-05,
      "loss": 0.4516,
      "step": 20000
    },
    {
      "epoch": 0.8670642473459375,
      "grad_norm": 4.286369800567627,
      "learning_rate": 1.4219853656473378e-05,
      "loss": 0.4336,
      "step": 20500
    },
    {
      "epoch": 0.8882121558177897,
      "grad_norm": 2.7780303955078125,
      "learning_rate": 1.4078867599994362e-05,
      "loss": 0.4426,
      "step": 21000
    },
    {
      "epoch": 0.9093600642896418,
      "grad_norm": 1.9510722160339355,
      "learning_rate": 1.3937881543515349e-05,
      "loss": 0.4482,
      "step": 21500
    },
    {
      "epoch": 0.9305079727614939,
      "grad_norm": 5.089450359344482,
      "learning_rate": 1.3796895487036333e-05,
      "loss": 0.4455,
      "step": 22000
    },
    {
      "epoch": 0.951655881233346,
      "grad_norm": 3.3516769409179688,
      "learning_rate": 1.3655909430557319e-05,
      "loss": 0.4306,
      "step": 22500
    },
    {
      "epoch": 0.9728037897051982,
      "grad_norm": 5.436390399932861,
      "learning_rate": 1.3514923374078304e-05,
      "loss": 0.4371,
      "step": 23000
    },
    {
      "epoch": 0.9939516981770503,
      "grad_norm": 3.571840763092041,
      "learning_rate": 1.337393731759929e-05,
      "loss": 0.4392,
      "step": 23500
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.8176784576163161,
      "eval_loss": 0.4282248616218567,
      "eval_runtime": 127.0488,
      "eval_samples_per_second": 372.18,
      "eval_steps_per_second": 23.267,
      "step": 23643
    }
  ],
  "logging_steps": 500,
  "max_steps": 70929,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.5147820510021504e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
