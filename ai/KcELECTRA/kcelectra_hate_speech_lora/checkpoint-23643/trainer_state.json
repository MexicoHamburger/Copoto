{
  "best_global_step": 23643,
  "best_metric": 0.3483840823173523,
  "best_model_checkpoint": "./kcelectra_combined_hate_speech_lora/checkpoint-23643",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 23643,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.021147908471852134,
      "grad_norm": 1.5356295108795166,
      "learning_rate": 1.9859295915633943e-05,
      "loss": 0.6262,
      "step": 500
    },
    {
      "epoch": 0.04229581694370427,
      "grad_norm": 1.6998625993728638,
      "learning_rate": 1.9718309859154933e-05,
      "loss": 0.4875,
      "step": 1000
    },
    {
      "epoch": 0.0634437254155564,
      "grad_norm": 2.2032854557037354,
      "learning_rate": 1.9577323802675915e-05,
      "loss": 0.4165,
      "step": 1500
    },
    {
      "epoch": 0.08459163388740853,
      "grad_norm": 6.067046165466309,
      "learning_rate": 1.9436337746196904e-05,
      "loss": 0.4051,
      "step": 2000
    },
    {
      "epoch": 0.10573954235926067,
      "grad_norm": 3.0957765579223633,
      "learning_rate": 1.929535168971789e-05,
      "loss": 0.3927,
      "step": 2500
    },
    {
      "epoch": 0.1268874508311128,
      "grad_norm": 2.0420312881469727,
      "learning_rate": 1.9154365633238875e-05,
      "loss": 0.3988,
      "step": 3000
    },
    {
      "epoch": 0.14803535930296494,
      "grad_norm": 1.6712923049926758,
      "learning_rate": 1.901337957675986e-05,
      "loss": 0.3918,
      "step": 3500
    },
    {
      "epoch": 0.16918326777481707,
      "grad_norm": 2.595897912979126,
      "learning_rate": 1.8872393520280847e-05,
      "loss": 0.3832,
      "step": 4000
    },
    {
      "epoch": 0.1903311762466692,
      "grad_norm": 3.2147421836853027,
      "learning_rate": 1.8731407463801832e-05,
      "loss": 0.3775,
      "step": 4500
    },
    {
      "epoch": 0.21147908471852134,
      "grad_norm": 4.223299980163574,
      "learning_rate": 1.8590421407322815e-05,
      "loss": 0.3935,
      "step": 5000
    },
    {
      "epoch": 0.23262699319037347,
      "grad_norm": 2.537612199783325,
      "learning_rate": 1.8449435350843804e-05,
      "loss": 0.3784,
      "step": 5500
    },
    {
      "epoch": 0.2537749016622256,
      "grad_norm": 4.216114044189453,
      "learning_rate": 1.830844929436479e-05,
      "loss": 0.3628,
      "step": 6000
    },
    {
      "epoch": 0.27492281013407777,
      "grad_norm": 3.225149154663086,
      "learning_rate": 1.8167463237885775e-05,
      "loss": 0.3674,
      "step": 6500
    },
    {
      "epoch": 0.2960707186059299,
      "grad_norm": 3.280853509902954,
      "learning_rate": 1.802647718140676e-05,
      "loss": 0.3806,
      "step": 7000
    },
    {
      "epoch": 0.31721862707778203,
      "grad_norm": 2.05039644241333,
      "learning_rate": 1.7885491124927746e-05,
      "loss": 0.3709,
      "step": 7500
    },
    {
      "epoch": 0.33836653554963414,
      "grad_norm": 3.4430527687072754,
      "learning_rate": 1.7744505068448732e-05,
      "loss": 0.3721,
      "step": 8000
    },
    {
      "epoch": 0.3595144440214863,
      "grad_norm": 2.5388708114624023,
      "learning_rate": 1.7603519011969718e-05,
      "loss": 0.3641,
      "step": 8500
    },
    {
      "epoch": 0.3806623524933384,
      "grad_norm": 2.362725019454956,
      "learning_rate": 1.7462532955490703e-05,
      "loss": 0.3685,
      "step": 9000
    },
    {
      "epoch": 0.40181026096519057,
      "grad_norm": 1.9854092597961426,
      "learning_rate": 1.732154689901169e-05,
      "loss": 0.3629,
      "step": 9500
    },
    {
      "epoch": 0.4229581694370427,
      "grad_norm": 3.076672315597534,
      "learning_rate": 1.7180560842532675e-05,
      "loss": 0.3535,
      "step": 10000
    },
    {
      "epoch": 0.44410607790889484,
      "grad_norm": 2.375584125518799,
      "learning_rate": 1.703957478605366e-05,
      "loss": 0.3507,
      "step": 10500
    },
    {
      "epoch": 0.46525398638074694,
      "grad_norm": 3.2985193729400635,
      "learning_rate": 1.6898588729574646e-05,
      "loss": 0.3587,
      "step": 11000
    },
    {
      "epoch": 0.4864018948525991,
      "grad_norm": 14.179441452026367,
      "learning_rate": 1.6757602673095632e-05,
      "loss": 0.3559,
      "step": 11500
    },
    {
      "epoch": 0.5075498033244512,
      "grad_norm": 3.1327409744262695,
      "learning_rate": 1.6616616616616618e-05,
      "loss": 0.3791,
      "step": 12000
    },
    {
      "epoch": 0.5286977117963033,
      "grad_norm": 2.221705675125122,
      "learning_rate": 1.6475630560137603e-05,
      "loss": 0.3521,
      "step": 12500
    },
    {
      "epoch": 0.5498456202681555,
      "grad_norm": 2.5586040019989014,
      "learning_rate": 1.633464450365859e-05,
      "loss": 0.3609,
      "step": 13000
    },
    {
      "epoch": 0.5709935287400076,
      "grad_norm": 1.545999526977539,
      "learning_rate": 1.6193658447179575e-05,
      "loss": 0.3509,
      "step": 13500
    },
    {
      "epoch": 0.5921414372118597,
      "grad_norm": 4.784600734710693,
      "learning_rate": 1.605267239070056e-05,
      "loss": 0.3522,
      "step": 14000
    },
    {
      "epoch": 0.6132893456837119,
      "grad_norm": 2.4965572357177734,
      "learning_rate": 1.591168633422155e-05,
      "loss": 0.3525,
      "step": 14500
    },
    {
      "epoch": 0.6344372541555641,
      "grad_norm": 2.896984577178955,
      "learning_rate": 1.577070027774253e-05,
      "loss": 0.3578,
      "step": 15000
    },
    {
      "epoch": 0.6555851626274162,
      "grad_norm": 3.2749335765838623,
      "learning_rate": 1.5629714221263517e-05,
      "loss": 0.359,
      "step": 15500
    },
    {
      "epoch": 0.6767330710992683,
      "grad_norm": 2.4982948303222656,
      "learning_rate": 1.5488728164784503e-05,
      "loss": 0.3614,
      "step": 16000
    },
    {
      "epoch": 0.6978809795711204,
      "grad_norm": 1.7029633522033691,
      "learning_rate": 1.534774210830549e-05,
      "loss": 0.3637,
      "step": 16500
    },
    {
      "epoch": 0.7190288880429726,
      "grad_norm": 1.039139747619629,
      "learning_rate": 1.5206756051826474e-05,
      "loss": 0.3481,
      "step": 17000
    },
    {
      "epoch": 0.7401767965148247,
      "grad_norm": 3.3578054904937744,
      "learning_rate": 1.5065769995347462e-05,
      "loss": 0.3583,
      "step": 17500
    },
    {
      "epoch": 0.7613247049866768,
      "grad_norm": 3.022155523300171,
      "learning_rate": 1.4924783938868447e-05,
      "loss": 0.358,
      "step": 18000
    },
    {
      "epoch": 0.7824726134585289,
      "grad_norm": 2.39398193359375,
      "learning_rate": 1.4783797882389433e-05,
      "loss": 0.3469,
      "step": 18500
    },
    {
      "epoch": 0.8036205219303811,
      "grad_norm": 2.1063950061798096,
      "learning_rate": 1.4642811825910419e-05,
      "loss": 0.3749,
      "step": 19000
    },
    {
      "epoch": 0.8247684304022332,
      "grad_norm": 2.5426383018493652,
      "learning_rate": 1.4501825769431403e-05,
      "loss": 0.3581,
      "step": 19500
    },
    {
      "epoch": 0.8459163388740853,
      "grad_norm": 2.052943706512451,
      "learning_rate": 1.436083971295239e-05,
      "loss": 0.3632,
      "step": 20000
    },
    {
      "epoch": 0.8670642473459375,
      "grad_norm": 2.9479386806488037,
      "learning_rate": 1.4219853656473378e-05,
      "loss": 0.3444,
      "step": 20500
    },
    {
      "epoch": 0.8882121558177897,
      "grad_norm": 3.3860883712768555,
      "learning_rate": 1.4078867599994362e-05,
      "loss": 0.3599,
      "step": 21000
    },
    {
      "epoch": 0.9093600642896418,
      "grad_norm": 1.9791229963302612,
      "learning_rate": 1.3937881543515349e-05,
      "loss": 0.3591,
      "step": 21500
    },
    {
      "epoch": 0.9305079727614939,
      "grad_norm": 4.9732537269592285,
      "learning_rate": 1.3796895487036333e-05,
      "loss": 0.3523,
      "step": 22000
    },
    {
      "epoch": 0.951655881233346,
      "grad_norm": 3.329800605773926,
      "learning_rate": 1.3655909430557319e-05,
      "loss": 0.3423,
      "step": 22500
    },
    {
      "epoch": 0.9728037897051982,
      "grad_norm": 3.4267637729644775,
      "learning_rate": 1.3514923374078304e-05,
      "loss": 0.3494,
      "step": 23000
    },
    {
      "epoch": 0.9939516981770503,
      "grad_norm": 3.2090256214141846,
      "learning_rate": 1.337393731759929e-05,
      "loss": 0.3571,
      "step": 23500
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.8645548880393228,
      "eval_loss": 0.3483840823173523,
      "eval_runtime": 95.0055,
      "eval_samples_per_second": 497.708,
      "eval_steps_per_second": 31.114,
      "step": 23643
    }
  ],
  "logging_steps": 500,
  "max_steps": 70929,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.1005295291209504e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
