{
  "best_global_step": 70929,
  "best_metric": 0.33811312913894653,
  "best_model_checkpoint": "./kcelectra_combined_hate_speech_lora/checkpoint-70929",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 70929,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.021147908471852134,
      "grad_norm": 1.5356295108795166,
      "learning_rate": 1.9859295915633943e-05,
      "loss": 0.6262,
      "step": 500
    },
    {
      "epoch": 0.04229581694370427,
      "grad_norm": 1.6998625993728638,
      "learning_rate": 1.9718309859154933e-05,
      "loss": 0.4875,
      "step": 1000
    },
    {
      "epoch": 0.0634437254155564,
      "grad_norm": 2.2032854557037354,
      "learning_rate": 1.9577323802675915e-05,
      "loss": 0.4165,
      "step": 1500
    },
    {
      "epoch": 0.08459163388740853,
      "grad_norm": 6.067046165466309,
      "learning_rate": 1.9436337746196904e-05,
      "loss": 0.4051,
      "step": 2000
    },
    {
      "epoch": 0.10573954235926067,
      "grad_norm": 3.0957765579223633,
      "learning_rate": 1.929535168971789e-05,
      "loss": 0.3927,
      "step": 2500
    },
    {
      "epoch": 0.1268874508311128,
      "grad_norm": 2.0420312881469727,
      "learning_rate": 1.9154365633238875e-05,
      "loss": 0.3988,
      "step": 3000
    },
    {
      "epoch": 0.14803535930296494,
      "grad_norm": 1.6712923049926758,
      "learning_rate": 1.901337957675986e-05,
      "loss": 0.3918,
      "step": 3500
    },
    {
      "epoch": 0.16918326777481707,
      "grad_norm": 2.595897912979126,
      "learning_rate": 1.8872393520280847e-05,
      "loss": 0.3832,
      "step": 4000
    },
    {
      "epoch": 0.1903311762466692,
      "grad_norm": 3.2147421836853027,
      "learning_rate": 1.8731407463801832e-05,
      "loss": 0.3775,
      "step": 4500
    },
    {
      "epoch": 0.21147908471852134,
      "grad_norm": 4.223299980163574,
      "learning_rate": 1.8590421407322815e-05,
      "loss": 0.3935,
      "step": 5000
    },
    {
      "epoch": 0.23262699319037347,
      "grad_norm": 2.537612199783325,
      "learning_rate": 1.8449435350843804e-05,
      "loss": 0.3784,
      "step": 5500
    },
    {
      "epoch": 0.2537749016622256,
      "grad_norm": 4.216114044189453,
      "learning_rate": 1.830844929436479e-05,
      "loss": 0.3628,
      "step": 6000
    },
    {
      "epoch": 0.27492281013407777,
      "grad_norm": 3.225149154663086,
      "learning_rate": 1.8167463237885775e-05,
      "loss": 0.3674,
      "step": 6500
    },
    {
      "epoch": 0.2960707186059299,
      "grad_norm": 3.280853509902954,
      "learning_rate": 1.802647718140676e-05,
      "loss": 0.3806,
      "step": 7000
    },
    {
      "epoch": 0.31721862707778203,
      "grad_norm": 2.05039644241333,
      "learning_rate": 1.7885491124927746e-05,
      "loss": 0.3709,
      "step": 7500
    },
    {
      "epoch": 0.33836653554963414,
      "grad_norm": 3.4430527687072754,
      "learning_rate": 1.7744505068448732e-05,
      "loss": 0.3721,
      "step": 8000
    },
    {
      "epoch": 0.3595144440214863,
      "grad_norm": 2.5388708114624023,
      "learning_rate": 1.7603519011969718e-05,
      "loss": 0.3641,
      "step": 8500
    },
    {
      "epoch": 0.3806623524933384,
      "grad_norm": 2.362725019454956,
      "learning_rate": 1.7462532955490703e-05,
      "loss": 0.3685,
      "step": 9000
    },
    {
      "epoch": 0.40181026096519057,
      "grad_norm": 1.9854092597961426,
      "learning_rate": 1.732154689901169e-05,
      "loss": 0.3629,
      "step": 9500
    },
    {
      "epoch": 0.4229581694370427,
      "grad_norm": 3.076672315597534,
      "learning_rate": 1.7180560842532675e-05,
      "loss": 0.3535,
      "step": 10000
    },
    {
      "epoch": 0.44410607790889484,
      "grad_norm": 2.375584125518799,
      "learning_rate": 1.703957478605366e-05,
      "loss": 0.3507,
      "step": 10500
    },
    {
      "epoch": 0.46525398638074694,
      "grad_norm": 3.2985193729400635,
      "learning_rate": 1.6898588729574646e-05,
      "loss": 0.3587,
      "step": 11000
    },
    {
      "epoch": 0.4864018948525991,
      "grad_norm": 14.179441452026367,
      "learning_rate": 1.6757602673095632e-05,
      "loss": 0.3559,
      "step": 11500
    },
    {
      "epoch": 0.5075498033244512,
      "grad_norm": 3.1327409744262695,
      "learning_rate": 1.6616616616616618e-05,
      "loss": 0.3791,
      "step": 12000
    },
    {
      "epoch": 0.5286977117963033,
      "grad_norm": 2.221705675125122,
      "learning_rate": 1.6475630560137603e-05,
      "loss": 0.3521,
      "step": 12500
    },
    {
      "epoch": 0.5498456202681555,
      "grad_norm": 2.5586040019989014,
      "learning_rate": 1.633464450365859e-05,
      "loss": 0.3609,
      "step": 13000
    },
    {
      "epoch": 0.5709935287400076,
      "grad_norm": 1.545999526977539,
      "learning_rate": 1.6193658447179575e-05,
      "loss": 0.3509,
      "step": 13500
    },
    {
      "epoch": 0.5921414372118597,
      "grad_norm": 4.784600734710693,
      "learning_rate": 1.605267239070056e-05,
      "loss": 0.3522,
      "step": 14000
    },
    {
      "epoch": 0.6132893456837119,
      "grad_norm": 2.4965572357177734,
      "learning_rate": 1.591168633422155e-05,
      "loss": 0.3525,
      "step": 14500
    },
    {
      "epoch": 0.6344372541555641,
      "grad_norm": 2.896984577178955,
      "learning_rate": 1.577070027774253e-05,
      "loss": 0.3578,
      "step": 15000
    },
    {
      "epoch": 0.6555851626274162,
      "grad_norm": 3.2749335765838623,
      "learning_rate": 1.5629714221263517e-05,
      "loss": 0.359,
      "step": 15500
    },
    {
      "epoch": 0.6767330710992683,
      "grad_norm": 2.4982948303222656,
      "learning_rate": 1.5488728164784503e-05,
      "loss": 0.3614,
      "step": 16000
    },
    {
      "epoch": 0.6978809795711204,
      "grad_norm": 1.7029633522033691,
      "learning_rate": 1.534774210830549e-05,
      "loss": 0.3637,
      "step": 16500
    },
    {
      "epoch": 0.7190288880429726,
      "grad_norm": 1.039139747619629,
      "learning_rate": 1.5206756051826474e-05,
      "loss": 0.3481,
      "step": 17000
    },
    {
      "epoch": 0.7401767965148247,
      "grad_norm": 3.3578054904937744,
      "learning_rate": 1.5065769995347462e-05,
      "loss": 0.3583,
      "step": 17500
    },
    {
      "epoch": 0.7613247049866768,
      "grad_norm": 3.022155523300171,
      "learning_rate": 1.4924783938868447e-05,
      "loss": 0.358,
      "step": 18000
    },
    {
      "epoch": 0.7824726134585289,
      "grad_norm": 2.39398193359375,
      "learning_rate": 1.4783797882389433e-05,
      "loss": 0.3469,
      "step": 18500
    },
    {
      "epoch": 0.8036205219303811,
      "grad_norm": 2.1063950061798096,
      "learning_rate": 1.4642811825910419e-05,
      "loss": 0.3749,
      "step": 19000
    },
    {
      "epoch": 0.8247684304022332,
      "grad_norm": 2.5426383018493652,
      "learning_rate": 1.4501825769431403e-05,
      "loss": 0.3581,
      "step": 19500
    },
    {
      "epoch": 0.8459163388740853,
      "grad_norm": 2.052943706512451,
      "learning_rate": 1.436083971295239e-05,
      "loss": 0.3632,
      "step": 20000
    },
    {
      "epoch": 0.8670642473459375,
      "grad_norm": 2.9479386806488037,
      "learning_rate": 1.4219853656473378e-05,
      "loss": 0.3444,
      "step": 20500
    },
    {
      "epoch": 0.8882121558177897,
      "grad_norm": 3.3860883712768555,
      "learning_rate": 1.4078867599994362e-05,
      "loss": 0.3599,
      "step": 21000
    },
    {
      "epoch": 0.9093600642896418,
      "grad_norm": 1.9791229963302612,
      "learning_rate": 1.3937881543515349e-05,
      "loss": 0.3591,
      "step": 21500
    },
    {
      "epoch": 0.9305079727614939,
      "grad_norm": 4.9732537269592285,
      "learning_rate": 1.3796895487036333e-05,
      "loss": 0.3523,
      "step": 22000
    },
    {
      "epoch": 0.951655881233346,
      "grad_norm": 3.329800605773926,
      "learning_rate": 1.3655909430557319e-05,
      "loss": 0.3423,
      "step": 22500
    },
    {
      "epoch": 0.9728037897051982,
      "grad_norm": 3.4267637729644775,
      "learning_rate": 1.3514923374078304e-05,
      "loss": 0.3494,
      "step": 23000
    },
    {
      "epoch": 0.9939516981770503,
      "grad_norm": 3.2090256214141846,
      "learning_rate": 1.337393731759929e-05,
      "loss": 0.3571,
      "step": 23500
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.8645548880393228,
      "eval_loss": 0.3483840823173523,
      "eval_runtime": 95.0055,
      "eval_samples_per_second": 497.708,
      "eval_steps_per_second": 31.114,
      "step": 23643
    },
    {
      "epoch": 1.0150996066489024,
      "grad_norm": 2.4147260189056396,
      "learning_rate": 1.3232951261120277e-05,
      "loss": 0.3518,
      "step": 24000
    },
    {
      "epoch": 1.0362475151207546,
      "grad_norm": 1.2018927335739136,
      "learning_rate": 1.3091965204641261e-05,
      "loss": 0.3571,
      "step": 24500
    },
    {
      "epoch": 1.0573954235926066,
      "grad_norm": 2.0718536376953125,
      "learning_rate": 1.2950979148162249e-05,
      "loss": 0.3488,
      "step": 25000
    },
    {
      "epoch": 1.0785433320644588,
      "grad_norm": 3.868445634841919,
      "learning_rate": 1.2809993091683233e-05,
      "loss": 0.3545,
      "step": 25500
    },
    {
      "epoch": 1.0996912405363108,
      "grad_norm": 0.6965459585189819,
      "learning_rate": 1.266900703520422e-05,
      "loss": 0.3441,
      "step": 26000
    },
    {
      "epoch": 1.120839149008163,
      "grad_norm": 3.8034403324127197,
      "learning_rate": 1.2528020978725206e-05,
      "loss": 0.3547,
      "step": 26500
    },
    {
      "epoch": 1.1419870574800153,
      "grad_norm": 1.8204972743988037,
      "learning_rate": 1.238703492224619e-05,
      "loss": 0.3411,
      "step": 27000
    },
    {
      "epoch": 1.1631349659518673,
      "grad_norm": 2.764486074447632,
      "learning_rate": 1.2246048865767177e-05,
      "loss": 0.3465,
      "step": 27500
    },
    {
      "epoch": 1.1842828744237195,
      "grad_norm": 2.4358553886413574,
      "learning_rate": 1.2105062809288161e-05,
      "loss": 0.3486,
      "step": 28000
    },
    {
      "epoch": 1.2054307828955717,
      "grad_norm": 5.146897792816162,
      "learning_rate": 1.1964076752809148e-05,
      "loss": 0.344,
      "step": 28500
    },
    {
      "epoch": 1.2265786913674237,
      "grad_norm": 7.170365810394287,
      "learning_rate": 1.1823090696330132e-05,
      "loss": 0.3348,
      "step": 29000
    },
    {
      "epoch": 1.247726599839276,
      "grad_norm": 3.2964420318603516,
      "learning_rate": 1.168210463985112e-05,
      "loss": 0.3376,
      "step": 29500
    },
    {
      "epoch": 1.2688745083111281,
      "grad_norm": 1.7219207286834717,
      "learning_rate": 1.1541118583372105e-05,
      "loss": 0.3435,
      "step": 30000
    },
    {
      "epoch": 1.2900224167829801,
      "grad_norm": 1.7920124530792236,
      "learning_rate": 1.1400132526893091e-05,
      "loss": 0.349,
      "step": 30500
    },
    {
      "epoch": 1.3111703252548323,
      "grad_norm": 3.4561409950256348,
      "learning_rate": 1.1259146470414077e-05,
      "loss": 0.3462,
      "step": 31000
    },
    {
      "epoch": 1.3323182337266846,
      "grad_norm": 4.370491027832031,
      "learning_rate": 1.1118160413935063e-05,
      "loss": 0.3574,
      "step": 31500
    },
    {
      "epoch": 1.3534661421985366,
      "grad_norm": 2.572422742843628,
      "learning_rate": 1.0977174357456048e-05,
      "loss": 0.3569,
      "step": 32000
    },
    {
      "epoch": 1.3746140506703888,
      "grad_norm": 4.404078960418701,
      "learning_rate": 1.0836188300977036e-05,
      "loss": 0.3413,
      "step": 32500
    },
    {
      "epoch": 1.3957619591422408,
      "grad_norm": 3.7632083892822266,
      "learning_rate": 1.069520224449802e-05,
      "loss": 0.3508,
      "step": 33000
    },
    {
      "epoch": 1.416909867614093,
      "grad_norm": 2.9039714336395264,
      "learning_rate": 1.0554216188019007e-05,
      "loss": 0.3466,
      "step": 33500
    },
    {
      "epoch": 1.438057776085945,
      "grad_norm": 4.152955055236816,
      "learning_rate": 1.0413230131539991e-05,
      "loss": 0.3428,
      "step": 34000
    },
    {
      "epoch": 1.4592056845577972,
      "grad_norm": 2.421407699584961,
      "learning_rate": 1.0272244075060978e-05,
      "loss": 0.3459,
      "step": 34500
    },
    {
      "epoch": 1.4803535930296494,
      "grad_norm": 2.014456272125244,
      "learning_rate": 1.0131258018581962e-05,
      "loss": 0.3489,
      "step": 35000
    },
    {
      "epoch": 1.5015015015015014,
      "grad_norm": 4.556264400482178,
      "learning_rate": 9.990271962102948e-06,
      "loss": 0.3507,
      "step": 35500
    },
    {
      "epoch": 1.5226494099733536,
      "grad_norm": 1.9618936777114868,
      "learning_rate": 9.849285905623934e-06,
      "loss": 0.3347,
      "step": 36000
    },
    {
      "epoch": 1.5437973184452058,
      "grad_norm": 2.4402918815612793,
      "learning_rate": 9.70829984914492e-06,
      "loss": 0.3444,
      "step": 36500
    },
    {
      "epoch": 1.5649452269170578,
      "grad_norm": 4.439415454864502,
      "learning_rate": 9.567313792665907e-06,
      "loss": 0.3452,
      "step": 37000
    },
    {
      "epoch": 1.58609313538891,
      "grad_norm": 3.0350422859191895,
      "learning_rate": 9.426327736186892e-06,
      "loss": 0.343,
      "step": 37500
    },
    {
      "epoch": 1.6072410438607623,
      "grad_norm": 5.192998886108398,
      "learning_rate": 9.285341679707878e-06,
      "loss": 0.3495,
      "step": 38000
    },
    {
      "epoch": 1.6283889523326143,
      "grad_norm": 3.4108614921569824,
      "learning_rate": 9.144355623228864e-06,
      "loss": 0.3353,
      "step": 38500
    },
    {
      "epoch": 1.6495368608044665,
      "grad_norm": 1.7296957969665527,
      "learning_rate": 9.00336956674985e-06,
      "loss": 0.336,
      "step": 39000
    },
    {
      "epoch": 1.6706847692763187,
      "grad_norm": 3.199413537979126,
      "learning_rate": 8.862383510270835e-06,
      "loss": 0.3404,
      "step": 39500
    },
    {
      "epoch": 1.6918326777481707,
      "grad_norm": 3.6242117881774902,
      "learning_rate": 8.72139745379182e-06,
      "loss": 0.3503,
      "step": 40000
    },
    {
      "epoch": 1.7129805862200227,
      "grad_norm": 3.4924354553222656,
      "learning_rate": 8.580411397312807e-06,
      "loss": 0.3279,
      "step": 40500
    },
    {
      "epoch": 1.7341284946918751,
      "grad_norm": 3.8714590072631836,
      "learning_rate": 8.439425340833792e-06,
      "loss": 0.3537,
      "step": 41000
    },
    {
      "epoch": 1.7552764031637271,
      "grad_norm": 2.895892381668091,
      "learning_rate": 8.298439284354778e-06,
      "loss": 0.3425,
      "step": 41500
    },
    {
      "epoch": 1.7764243116355791,
      "grad_norm": 2.585906982421875,
      "learning_rate": 8.157453227875764e-06,
      "loss": 0.3506,
      "step": 42000
    },
    {
      "epoch": 1.7975722201074313,
      "grad_norm": 3.5350210666656494,
      "learning_rate": 8.01646717139675e-06,
      "loss": 0.3491,
      "step": 42500
    },
    {
      "epoch": 1.8187201285792836,
      "grad_norm": 2.9514262676239014,
      "learning_rate": 7.875481114917735e-06,
      "loss": 0.3442,
      "step": 43000
    },
    {
      "epoch": 1.8398680370511356,
      "grad_norm": 2.1787447929382324,
      "learning_rate": 7.73449505843872e-06,
      "loss": 0.3448,
      "step": 43500
    },
    {
      "epoch": 1.8610159455229878,
      "grad_norm": 2.5739517211914062,
      "learning_rate": 7.593509001959707e-06,
      "loss": 0.3528,
      "step": 44000
    },
    {
      "epoch": 1.88216385399484,
      "grad_norm": 1.5072335004806519,
      "learning_rate": 7.452522945480693e-06,
      "loss": 0.3425,
      "step": 44500
    },
    {
      "epoch": 1.903311762466692,
      "grad_norm": 3.552885055541992,
      "learning_rate": 7.311536889001678e-06,
      "loss": 0.3321,
      "step": 45000
    },
    {
      "epoch": 1.9244596709385442,
      "grad_norm": 2.7979934215545654,
      "learning_rate": 7.170550832522663e-06,
      "loss": 0.347,
      "step": 45500
    },
    {
      "epoch": 1.9456075794103964,
      "grad_norm": 2.2513837814331055,
      "learning_rate": 7.029564776043651e-06,
      "loss": 0.342,
      "step": 46000
    },
    {
      "epoch": 1.9667554878822484,
      "grad_norm": 7.019099712371826,
      "learning_rate": 6.8885787195646355e-06,
      "loss": 0.3523,
      "step": 46500
    },
    {
      "epoch": 1.9879033963541006,
      "grad_norm": 2.4904425144195557,
      "learning_rate": 6.747592663085621e-06,
      "loss": 0.3378,
      "step": 47000
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.8671242384514537,
      "eval_loss": 0.34514865279197693,
      "eval_runtime": 97.6922,
      "eval_samples_per_second": 484.02,
      "eval_steps_per_second": 30.258,
      "step": 47286
    },
    {
      "epoch": 2.009051304825953,
      "grad_norm": 1.7040319442749023,
      "learning_rate": 6.606606606606607e-06,
      "loss": 0.3384,
      "step": 47500
    },
    {
      "epoch": 2.030199213297805,
      "grad_norm": 3.497565984725952,
      "learning_rate": 6.465620550127593e-06,
      "loss": 0.341,
      "step": 48000
    },
    {
      "epoch": 2.051347121769657,
      "grad_norm": 2.6415023803710938,
      "learning_rate": 6.324634493648578e-06,
      "loss": 0.3511,
      "step": 48500
    },
    {
      "epoch": 2.0724950302415093,
      "grad_norm": 3.9460561275482178,
      "learning_rate": 6.183648437169565e-06,
      "loss": 0.3434,
      "step": 49000
    },
    {
      "epoch": 2.0936429387133613,
      "grad_norm": 1.8286993503570557,
      "learning_rate": 6.0426623806905505e-06,
      "loss": 0.3488,
      "step": 49500
    },
    {
      "epoch": 2.1147908471852133,
      "grad_norm": 1.7152998447418213,
      "learning_rate": 5.901676324211536e-06,
      "loss": 0.3401,
      "step": 50000
    },
    {
      "epoch": 2.1359387556570657,
      "grad_norm": 3.7368247509002686,
      "learning_rate": 5.760690267732522e-06,
      "loss": 0.3265,
      "step": 50500
    },
    {
      "epoch": 2.1570866641289177,
      "grad_norm": 2.362797975540161,
      "learning_rate": 5.6197042112535075e-06,
      "loss": 0.3402,
      "step": 51000
    },
    {
      "epoch": 2.1782345726007697,
      "grad_norm": 3.687408208847046,
      "learning_rate": 5.478718154774492e-06,
      "loss": 0.3465,
      "step": 51500
    },
    {
      "epoch": 2.1993824810726217,
      "grad_norm": 3.043475389480591,
      "learning_rate": 5.33773209829548e-06,
      "loss": 0.3368,
      "step": 52000
    },
    {
      "epoch": 2.220530389544474,
      "grad_norm": 3.149956464767456,
      "learning_rate": 5.196746041816465e-06,
      "loss": 0.338,
      "step": 52500
    },
    {
      "epoch": 2.241678298016326,
      "grad_norm": 3.272087812423706,
      "learning_rate": 5.05575998533745e-06,
      "loss": 0.3472,
      "step": 53000
    },
    {
      "epoch": 2.262826206488178,
      "grad_norm": 3.0343666076660156,
      "learning_rate": 4.914773928858436e-06,
      "loss": 0.3391,
      "step": 53500
    },
    {
      "epoch": 2.2839741149600306,
      "grad_norm": 2.6750454902648926,
      "learning_rate": 4.7737878723794225e-06,
      "loss": 0.3493,
      "step": 54000
    },
    {
      "epoch": 2.3051220234318825,
      "grad_norm": 3.5602829456329346,
      "learning_rate": 4.632801815900408e-06,
      "loss": 0.3398,
      "step": 54500
    },
    {
      "epoch": 2.3262699319037345,
      "grad_norm": 5.897075176239014,
      "learning_rate": 4.491815759421394e-06,
      "loss": 0.3396,
      "step": 55000
    },
    {
      "epoch": 2.347417840375587,
      "grad_norm": 3.2371790409088135,
      "learning_rate": 4.3508297029423795e-06,
      "loss": 0.3531,
      "step": 55500
    },
    {
      "epoch": 2.368565748847439,
      "grad_norm": 2.5262644290924072,
      "learning_rate": 4.209843646463365e-06,
      "loss": 0.3307,
      "step": 56000
    },
    {
      "epoch": 2.389713657319291,
      "grad_norm": 1.58597731590271,
      "learning_rate": 4.068857589984351e-06,
      "loss": 0.3214,
      "step": 56500
    },
    {
      "epoch": 2.4108615657911434,
      "grad_norm": 1.9793181419372559,
      "learning_rate": 3.9278715335053366e-06,
      "loss": 0.3435,
      "step": 57000
    },
    {
      "epoch": 2.4320094742629954,
      "grad_norm": 2.42226243019104,
      "learning_rate": 3.7868854770263227e-06,
      "loss": 0.3306,
      "step": 57500
    },
    {
      "epoch": 2.4531573827348474,
      "grad_norm": 4.222668647766113,
      "learning_rate": 3.645899420547308e-06,
      "loss": 0.3331,
      "step": 58000
    },
    {
      "epoch": 2.4743052912067,
      "grad_norm": 3.7491495609283447,
      "learning_rate": 3.504913364068294e-06,
      "loss": 0.3517,
      "step": 58500
    },
    {
      "epoch": 2.495453199678552,
      "grad_norm": 3.5809121131896973,
      "learning_rate": 3.3639273075892797e-06,
      "loss": 0.329,
      "step": 59000
    },
    {
      "epoch": 2.516601108150404,
      "grad_norm": 5.589010238647461,
      "learning_rate": 3.2229412511102654e-06,
      "loss": 0.3355,
      "step": 59500
    },
    {
      "epoch": 2.5377490166222563,
      "grad_norm": 2.1285390853881836,
      "learning_rate": 3.0819551946312515e-06,
      "loss": 0.3452,
      "step": 60000
    },
    {
      "epoch": 2.5588969250941083,
      "grad_norm": 2.8753788471221924,
      "learning_rate": 2.940969138152237e-06,
      "loss": 0.3339,
      "step": 60500
    },
    {
      "epoch": 2.5800448335659603,
      "grad_norm": 4.878556251525879,
      "learning_rate": 2.7999830816732224e-06,
      "loss": 0.34,
      "step": 61000
    },
    {
      "epoch": 2.6011927420378127,
      "grad_norm": 2.4342730045318604,
      "learning_rate": 2.658997025194209e-06,
      "loss": 0.3505,
      "step": 61500
    },
    {
      "epoch": 2.6223406505096647,
      "grad_norm": 3.1328694820404053,
      "learning_rate": 2.5180109687151942e-06,
      "loss": 0.3367,
      "step": 62000
    },
    {
      "epoch": 2.6434885589815167,
      "grad_norm": 1.8945326805114746,
      "learning_rate": 2.37702491223618e-06,
      "loss": 0.343,
      "step": 62500
    },
    {
      "epoch": 2.664636467453369,
      "grad_norm": 4.864308834075928,
      "learning_rate": 2.2360388557571656e-06,
      "loss": 0.3359,
      "step": 63000
    },
    {
      "epoch": 2.685784375925221,
      "grad_norm": 6.384109973907471,
      "learning_rate": 2.0950527992781517e-06,
      "loss": 0.3367,
      "step": 63500
    },
    {
      "epoch": 2.706932284397073,
      "grad_norm": 2.470932960510254,
      "learning_rate": 1.9540667427991374e-06,
      "loss": 0.345,
      "step": 64000
    },
    {
      "epoch": 2.728080192868925,
      "grad_norm": 1.593671202659607,
      "learning_rate": 1.813080686320123e-06,
      "loss": 0.3394,
      "step": 64500
    },
    {
      "epoch": 2.7492281013407776,
      "grad_norm": 2.550344944000244,
      "learning_rate": 1.672094629841109e-06,
      "loss": 0.3363,
      "step": 65000
    },
    {
      "epoch": 2.7703760098126295,
      "grad_norm": 5.460306167602539,
      "learning_rate": 1.5311085733620944e-06,
      "loss": 0.3472,
      "step": 65500
    },
    {
      "epoch": 2.7915239182844815,
      "grad_norm": 3.59733510017395,
      "learning_rate": 1.3901225168830803e-06,
      "loss": 0.3316,
      "step": 66000
    },
    {
      "epoch": 2.8126718267563335,
      "grad_norm": 3.427229404449463,
      "learning_rate": 1.249136460404066e-06,
      "loss": 0.3342,
      "step": 66500
    },
    {
      "epoch": 2.833819735228186,
      "grad_norm": 4.054324150085449,
      "learning_rate": 1.108150403925052e-06,
      "loss": 0.3421,
      "step": 67000
    },
    {
      "epoch": 2.854967643700038,
      "grad_norm": 5.04106330871582,
      "learning_rate": 9.671643474460376e-07,
      "loss": 0.3346,
      "step": 67500
    },
    {
      "epoch": 2.87611555217189,
      "grad_norm": 1.6947721242904663,
      "learning_rate": 8.261782909670234e-07,
      "loss": 0.3418,
      "step": 68000
    },
    {
      "epoch": 2.8972634606437424,
      "grad_norm": 3.6029651165008545,
      "learning_rate": 6.851922344880093e-07,
      "loss": 0.3353,
      "step": 68500
    },
    {
      "epoch": 2.9184113691155944,
      "grad_norm": 4.005532264709473,
      "learning_rate": 5.442061780089949e-07,
      "loss": 0.3366,
      "step": 69000
    },
    {
      "epoch": 2.9395592775874464,
      "grad_norm": 4.09225606918335,
      "learning_rate": 4.0322012152998073e-07,
      "loss": 0.3468,
      "step": 69500
    },
    {
      "epoch": 2.960707186059299,
      "grad_norm": 4.0108418464660645,
      "learning_rate": 2.6223406505096646e-07,
      "loss": 0.3392,
      "step": 70000
    },
    {
      "epoch": 2.981855094531151,
      "grad_norm": 1.114755630493164,
      "learning_rate": 1.2124800857195223e-07,
      "loss": 0.3396,
      "step": 70500
    },
    {
      "epoch": 3.0,
      "eval_f1": 0.8675277624635741,
      "eval_loss": 0.33811312913894653,
      "eval_runtime": 101.5211,
      "eval_samples_per_second": 465.765,
      "eval_steps_per_second": 29.117,
      "step": 70929
    }
  ],
  "logging_steps": 500,
  "max_steps": 70929,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.303664185384374e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
